{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:gainx] *",
      "language": "python",
      "name": "conda-env-gainx-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a246a2d-fc50-4e26-bb49-1ecd8b7c5fd0"
      },
      "source": [
        "# **Procesamiento del lenguaje natural**\n",
        "\n",
        "------\n",
        "\n",
        "\n",
        "El procesamiento del lenguaje natural (PLN) es la manipulación automática del lenguaje natural (como textos, discursos, etc.) por medio de algoritmos construidos para tal fin."
      ],
      "id": "1a246a2d-fc50-4e26-bb49-1ecd8b7c5fd0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "tags": [],
        "id": "f8ccef5c-a5cf-47d5-843f-fd46bfe96198"
      },
      "source": [
        "## Palabras\n",
        "--------------\n",
        "\n",
        "La mayoría de los modelos de procesamiento del lenguaje natural se basan en el estudio de las palabras. Esto se debe principalmente a que\n",
        "\n",
        "- Las **palabras** se pueden considerar como la **unidad básica** del lengugaje.\n",
        "\n",
        "- Existen **relaciones semánticas** entre las palabras que permiten pensar en relaciones de **similitud** y **distancia** entre las mismas.\n",
        "\n",
        "Por supuesto, no son las únicas alternativas. El lenguaje se podría pensar también analizando conjuntos de palabras, como frases o documentos enteros, o fragmentos de palabras."
      ],
      "id": "f8ccef5c-a5cf-47d5-843f-fd46bfe96198"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "63109d41-6464-4032-97cd-a3475c9cc55e"
      },
      "source": [
        "# **Extracción de Tokens**\n",
        "\n",
        "-------\n",
        "Lo primero que vamos a hacer es construir una lista de términos o **tokens** a partir de un dado texto."
      ],
      "id": "63109d41-6464-4032-97cd-a3475c9cc55e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b24420fc-07c1-4c38-8855-05cb16a86316",
        "outputId": "ec3a0cf1-0cb4-4156-a055-fe27b10ea678",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "text = (\n",
        "    'Muchos años después, frente al pelotón de fusilamiento, '\n",
        "    'el coronel Aureliano Buendía había de recordar aquella tarde remota '\n",
        "    'en que su padre lo llevó a conocer el hielo.'\n",
        ")\n",
        "\n",
        "## Transformamos el string en una lista de palabras\n",
        "tokens = text.split()\n",
        "tokens[:12]"
      ],
      "id": "b24420fc-07c1-4c38-8855-05cb16a86316",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Muchos',\n",
              " 'años',\n",
              " 'después,',\n",
              " 'frente',\n",
              " 'al',\n",
              " 'pelotón',\n",
              " 'de',\n",
              " 'fusilamiento,',\n",
              " 'el',\n",
              " 'coronel',\n",
              " 'Aureliano',\n",
              " 'Buendía']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f20d7cc-acca-44eb-94b6-589a929e71cc"
      },
      "source": [
        "Vemos que esta implementación naïve funciona relativamente bien, pero tiene algunos problemas. Por ejemplo, no reconoce los signos de puntuación."
      ],
      "id": "4f20d7cc-acca-44eb-94b6-589a929e71cc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71d5274e-bfb9-4ee3-854d-dff801ba52a5",
        "outputId": "e289c2bb-5eed-4e66-e766-a261c191ec18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "source": [
        "import spacy\n",
        "nlp = spacy.load('es_core_news_sm')\n",
        "\n",
        "doc = nlp(text)\n",
        "tokens = [token.text for token in doc]\n",
        "tokens[:14]"
      ],
      "id": "71d5274e-bfb9-4ee3-854d-dff801ba52a5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-69c744b79f60>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'es_core_news_sm'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0menable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0mexclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     )\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[1;32m    434\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[index]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'es_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "e4b5f6fb-ec8e-4755-a4e4-8aee6b832f1a"
      },
      "source": [
        "## Categoría gramatical (Part of Speech)\n",
        "\n",
        "-----------\n",
        "\n",
        "Una parte importante del procesamiento de texto es identificar a qué parte de la oración pertenece cada palabra. Por ejemplo, identificar verbos, sustantivos, artículos, adverbios, etc."
      ],
      "id": "e4b5f6fb-ec8e-4755-a4e4-8aee6b832f1a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aa17fa7b-5622-4738-9c6b-6e266749b0b8",
        "outputId": "bea6bffe-22e0-46af-84e8-67964304c2f7"
      },
      "source": [
        "[(token.text, token.pos_) for token in nlp('a la mañana yo camino por el camino')]"
      ],
      "id": "aa17fa7b-5622-4738-9c6b-6e266749b0b8",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('a', 'ADP'),\n",
              " ('la', 'DET'),\n",
              " ('mañana', 'NOUN'),\n",
              " ('yo', 'PRON'),\n",
              " ('camino', 'NOUN'),\n",
              " ('por', 'ADP'),\n",
              " ('el', 'DET'),\n",
              " ('camino', 'NOUN')]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c111fee0-4f74-42fb-95cc-7c9a6a6d527d",
        "outputId": "d4eaa140-7554-44de-f1ad-3672be39fc15"
      },
      "source": [
        "pos_tags = [(token.text, token.pos_) for token in doc]\n",
        "pos_tags[:14]"
      ],
      "id": "c111fee0-4f74-42fb-95cc-7c9a6a6d527d",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Muchos', 'DET'),\n",
              " ('años', 'NOUN'),\n",
              " ('después', 'ADV'),\n",
              " (',', 'PUNCT'),\n",
              " ('frente', 'NOUN'),\n",
              " ('al', 'ADP'),\n",
              " ('pelotón', 'NOUN'),\n",
              " ('de', 'ADP'),\n",
              " ('fusilamiento', 'NOUN'),\n",
              " (',', 'PUNCT'),\n",
              " ('el', 'DET'),\n",
              " ('coronel', 'NOUN'),\n",
              " ('Aureliano', 'PROPN'),\n",
              " ('Buendía', 'PROPN')]"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "41934b19-cc8a-4b39-907b-d9ad90545eac"
      },
      "source": [
        "## Lematización\n",
        "\n",
        "----------------\n",
        "\n",
        "En algunos casos, muchas palabras similares corresponden a un mismo concepto. Por ejemplo, en el español los adjetivos declinan según género y número (\"mucha\", \"mucho\", \"muchas\", \"muchos\")"
      ],
      "id": "41934b19-cc8a-4b39-907b-d9ad90545eac"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "788fa638-7c92-4c94-baa5-1dda5a5322e2",
        "outputId": "a7c681c1-20f8-45a2-98fd-5e849ada0953"
      },
      "source": [
        "lemmas = [(token.text, token.lemma_) for token in doc]\n",
        "lemmas[:14]"
      ],
      "id": "788fa638-7c92-4c94-baa5-1dda5a5322e2",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Muchos', 'mucho'),\n",
              " ('años', 'año'),\n",
              " ('después', 'después'),\n",
              " (',', ','),\n",
              " ('frente', 'frente'),\n",
              " ('al', 'al'),\n",
              " ('pelotón', 'pelotón'),\n",
              " ('de', 'de'),\n",
              " ('fusilamiento', 'fusilamiento'),\n",
              " (',', ','),\n",
              " ('el', 'el'),\n",
              " ('coronel', 'coronel'),\n",
              " ('Aureliano', 'Aureliano'),\n",
              " ('Buendía', 'Buendía')]"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "ee9cfef6-0ad8-4833-9f95-8f59ea12d0b7"
      },
      "source": [
        "## Extracción de términos relevantes\n",
        "\n",
        "------------\n",
        "\n",
        "Cuando trabajamos con palabras, nos interesan principalmente aquellas que representen conceptos relevantes. Por eso, es necesario excluir de nuestro análisis palabras muy frecuentes, como por ejemplo los artículos. También es conveniente descartar los signos de puntuación."
      ],
      "id": "ee9cfef6-0ad8-4833-9f95-8f59ea12d0b7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b649e90-bd58-4200-b4d8-e619f5d8f6fb",
        "outputId": "32dfb737-b74e-48c1-d2bb-87e336c1677e"
      },
      "source": [
        "def is_relevant_token(token):\n",
        "    if token.is_punct or token.is_stop:\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "def extract_relevant_tokens(text):\n",
        "    return [token for token in nlp(text) if is_relevant_token(token)]\n",
        "\n",
        "extract_relevant_tokens(text)"
      ],
      "id": "3b649e90-bd58-4200-b4d8-e619f5d8f6fb",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[años,\n",
              " frente,\n",
              " pelotón,\n",
              " fusilamiento,\n",
              " coronel,\n",
              " Aureliano,\n",
              " Buendía,\n",
              " recordar,\n",
              " remota,\n",
              " padre,\n",
              " llevó,\n",
              " a,\n",
              " hielo]"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7393a253-34df-4902-8858-8ffd3e380a24"
      },
      "source": [
        "Si ahora lematizamos, obtenemos una representación simplificada de nuestro texto original"
      ],
      "id": "7393a253-34df-4902-8858-8ffd3e380a24"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60b4cfcb-3c1c-47db-9afd-3fe75873175c",
        "outputId": "327b1479-da0b-47dc-f7a0-fe31e1ff8271"
      },
      "source": [
        "def lemmatize(text):\n",
        "    return [token.lemma_ for token in nlp(text) if is_relevant_token(token)]\n",
        "\n",
        "lemmatize(text)"
      ],
      "id": "60b4cfcb-3c1c-47db-9afd-3fe75873175c",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['año',\n",
              " 'frente',\n",
              " 'pelotón',\n",
              " 'fusilamiento',\n",
              " 'coronel',\n",
              " 'Aureliano',\n",
              " 'Buendía',\n",
              " 'recordar',\n",
              " 'remoto',\n",
              " 'padre',\n",
              " 'llevar',\n",
              " 'a',\n",
              " 'hielo']"
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "9f4e3f8b-bdcd-42cb-8fea-585ba341c7c1"
      },
      "source": [
        "## Reconocimiento de entidades nombradas\n",
        "\n",
        "-------------------\n",
        "\n",
        "Una entidad nombrada representa un objeto concreto del mundo real, como personas, organizaciones, lugares. La librería `spaCy` nos permite extraer estas entidades del texto"
      ],
      "id": "9f4e3f8b-bdcd-42cb-8fea-585ba341c7c1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "3b12e47c-727f-4ad3-963e-fa1dc45cc16f",
        "outputId": "9c86b826-3357-4572-b7fb-56b0c2402234"
      },
      "source": [
        "entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "entities"
      ],
      "id": "3b12e47c-727f-4ad3-963e-fa1dc45cc16f",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('Aureliano Buendía', 'PER')]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "4ed474e8-be03-4b6d-b2da-df8b1d8436a4"
      },
      "source": [
        "# **Representaciones de palabras (embeddings)**\n",
        "\n",
        "-------\n",
        "\n",
        "En Machine Learning, llamamos **embedding** a la representación de un objeto como un vector. Dado que venimos trabajando con palabras, buscaremos la manera de representar a las mismas como vectores."
      ],
      "id": "4ed474e8-be03-4b6d-b2da-df8b1d8436a4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fdb6f66-dcb2-4d77-acde-8cb0215cbd54"
      },
      "source": [
        "## One-hot encoding\n",
        "\n",
        "Es el embedding más sencillo de construir. Partimos de un cuerpo de texto, o *corpus*, y contamos la cantidad $N$ de palabras distintas. Esas palabras distintas constituyen nuestro alfabeto, o diccionario. Luego, a cada palabra distinta le asignamos un número $i$ entre $0$ y $N-1$, y le asociamos un vector con un $1$ en la posición $i$ y ceros en las demás posiciones.\n",
        "\n"
      ],
      "id": "9fdb6f66-dcb2-4d77-acde-8cb0215cbd54"
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "6dbeacf5-8008-4a16-9b51-760aef97cde6"
      },
      "source": [
        "def get_one_hot_embedding(text):\n",
        "    tokens = text.split(' ')\n",
        "    unique_words = set(text.split(' '))\n",
        "    N = len(unique_words)\n",
        "    word_to_int = {word: i for i, word in enumerate(unique_words)}\n",
        "    word_vectors = []\n",
        "    for word in tokens:\n",
        "        word_vec = [0] * N\n",
        "        i = word_to_int[word]\n",
        "        word_vec[i] = 1\n",
        "        word_vectors.append(word_vec)\n",
        "    return word_to_int, word_vectors"
      ],
      "id": "6dbeacf5-8008-4a16-9b51-760aef97cde6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "24c6cefb-b5c5-480e-832b-e2b741066389",
        "outputId": "78e9d401-cde2-4c9e-b298-e696db4c237a"
      },
      "source": [
        "text = \"Muchos años después, frente al pelotón\"\n",
        "word_to_int, word_vectors = get_one_hot_embedding(text)\n",
        "print(word_to_int)\n",
        "word_vectors"
      ],
      "id": "24c6cefb-b5c5-480e-832b-e2b741066389",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'pelotón': 0, 'al': 1, 'Muchos': 2, 'años': 3, 'frente': 4, 'después,': 5}\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[[0, 0, 1, 0, 0, 0],\n",
              " [0, 0, 0, 1, 0, 0],\n",
              " [0, 0, 0, 0, 0, 1],\n",
              " [0, 0, 0, 0, 1, 0],\n",
              " [0, 1, 0, 0, 0, 0],\n",
              " [1, 0, 0, 0, 0, 0]]"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fb39287-9a3b-4d17-8f04-783562c07acc"
      },
      "source": [
        "Como contraparte de su sencillez, este método tiene muchas desventajas:\n",
        "\n",
        "- Los vectores generados son ralos (tienen muchos ceros)\n",
        "\n",
        "- Si el corpus es extenso, los vectores son muy grandes\n",
        "\n",
        "- No refleja similitud ni cercanía de palabras\n",
        "\n",
        "- No identifican polisemia (misma palabra con más de un significado)"
      ],
      "id": "7fb39287-9a3b-4d17-8f04-783562c07acc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "50fe01a0-3e9e-483d-985f-48b561d1be4c"
      },
      "source": [
        "## Las palabras y su contexto\n",
        "\n",
        "El principal problema del one-hot encoding es que trata a las palabras en forma aislada, sin tener en cuenta su contexto.\n",
        "\n",
        "\n",
        "Ejemplo: (sacado de [acá](https://lena-voita.github.io/nlp_course/word_embeddings.html))\n",
        "\n",
        "¿Qué significa <span style=\"color:green\"><strong>**tezgüino**</strong></span>?\n",
        "\n",
        "Ni idea.\n",
        "\n",
        "Agreguemos algo de contexto:\n",
        "\n",
        "- Hay una botella de  <span style=\"color:green\"><strong>**tezgüino**</strong></span> sobre la mesa.\n",
        "\n",
        "- A todo el mundo le gusta el  <span style=\"color:green\"><strong>**tezgüino**</strong></span>.\n",
        "\n",
        "- El  <span style=\"color:green\"><strong>**tezgüino**</strong></span> te emborracha.\n",
        "\n",
        "- El <span style=\"color:green\"><strong>**tezgüino**</strong></span> se hace con maiz.\n",
        "\n",
        "Ahora podemos tener una idea de lo que significa la palabra (probablemente algún tipo de bebida alcohólica)."
      ],
      "id": "50fe01a0-3e9e-483d-985f-48b561d1be4c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0450228-59cf-4521-b1a2-6bd2b2610304"
      },
      "source": [
        "## Matriz de coocurrencia\n",
        "\n",
        "La matriz de coocurrencia es una matriz que permite incorporar la idea de contexto a nuestro análisis. Es una simétrica donde cada fila corresponde a una palabra, y donde el elemento $i,j$ indica la cantidad de veces en las que la palabra $i$ y la palabra $j$ ocurren cerca en un texto. Típicamente, \"cerca\" significa una ventana de tamaño fijo centrada en la palabra."
      ],
      "id": "b0450228-59cf-4521-b1a2-6bd2b2610304"
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "1b56423c-a13c-4c4b-bc69-33252e0a0856"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def coocurrence_matrix(text, w):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text for token in doc if not token.is_punct]\n",
        "    token2id = {token: i for i, token in enumerate(set(tokens))}\n",
        "    N = len(set(tokens))\n",
        "    matrix = np.zeros((N,N), dtype='int')\n",
        "    for idx, token in enumerate(tokens):\n",
        "        left_idx = max(0, idx-w)\n",
        "        right_idx = min(len(tokens)-1, idx+w)\n",
        "        window = tokens[left_idx:right_idx]\n",
        "        assert 0 < len(window) <= 2*w+1\n",
        "        for neighbor in window:\n",
        "            i, j = token2id[token], token2id[neighbor]\n",
        "            matrix[i,j] += 1\n",
        "            if i != j:\n",
        "                matrix[j,i] += 1\n",
        "    df = pd.DataFrame(matrix, columns=token2id.keys(), index=token2id.keys())\n",
        "    return df"
      ],
      "id": "1b56423c-a13c-4c4b-bc69-33252e0a0856",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b4519a2-689e-46a1-a983-857dce526b95",
        "outputId": "e6ad661a-ea1c-4b0c-8876-b9a02f131dc0"
      },
      "source": [
        "text = (\n",
        "'beautiful is better than ugly '\n",
        "'explicit is better than implicit '\n",
        "'simple is better than complex '\n",
        "'complex is better than complicated '\n",
        "'flat is better than nested '\n",
        "'sparse is better than dense '\n",
        ")\n",
        "\n",
        "df = coocurrence_matrix(text, w=2)\n",
        "print('Matriz de coocurrencia')\n",
        "df.iloc[:7, :7]"
      ],
      "id": "3b4519a2-689e-46a1-a983-857dce526b95",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Matriz de coocurrencia\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>simple</th>\n",
              "      <th>complex</th>\n",
              "      <th>dense</th>\n",
              "      <th>flat</th>\n",
              "      <th>than</th>\n",
              "      <th>sparse</th>\n",
              "      <th>explicit</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>simple</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>complex</th>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dense</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>flat</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>than</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sparse</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>explicit</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          simple  complex  dense  flat  than  sparse  explicit\n",
              "simple         1        0      0     0     1       0         0\n",
              "complex        0        4      0     0     3       0         0\n",
              "dense          0        0      0     0     1       0         0\n",
              "flat           0        0      0     1     1       0         0\n",
              "than           1        3      1     1     6       1         1\n",
              "sparse         0        0      0     0     1       1         0\n",
              "explicit       0        0      0     0     1       0         1"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c271bb34-39ec-400f-baab-f0ddbac2a952"
      },
      "source": [
        "Existen formas de normalizar esta matriz, teniendo en cuenta no sólo la coocurrencia de cada palabra, sino la frecuencia con la que es utilizada. Dos métodos usules de normalización son [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) y [PMI](https://en.wikipedia.org/wiki/Pointwise_mutual_information).\n",
        "\n",
        "La matriz de coocurrencia no resuelve el problema de la dimensión del embedding. Al igual que el one-hot encoding, la dimensión de cada vector es igual a la cantidad de palabras distintas. Esto se puede resolver haciendo una descomposición en valores singulares truncada, quedándonos con una cantidad acotada de vectores singulares.\n",
        "\n",
        "![img](https://miro.medium.com/max/700/1*0RbLKZe8HLg9-asCsHp_Jw.png)"
      ],
      "id": "c271bb34-39ec-400f-baab-f0ddbac2a952"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "af80c733-8083-485f-987f-4d0165010ac8"
      },
      "source": [
        "## Matriz de términos-documentos\n",
        "\n",
        "Supongamos que podemos dividir nuestro corpus en documentos. La definición de documento es laxa, y podría indicar un texto completo, o bien un párrafo o una oración. Para construir la matriz de términos-documentos colocamos cada palabra como fila y cada documento como columna. Así, el elemento $i,j$ de la matriz indica la cantidad de veces que la palabra $j$ ocurre dentro del documento $j$.\n",
        "\n",
        "![img](https://qph.fs.quoracdn.net/main-qimg-27639a9e2f88baab88a2c575a1de2005)"
      ],
      "id": "af80c733-8083-485f-987f-4d0165010ac8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "f76c842a-bc4a-48a2-b611-c326202a162d"
      },
      "source": [
        "## Word2Vec\n",
        "\n",
        "Word2Vec es un algoritmo de embedding en el cual se utiliza el contexto para generar una tarea predictiva, conocida como tarea de pretexto.\n",
        "   \n",
        "Lo primero que hacemos es deslizar una ventana sobre el texto, de manera tal de que, para cada palabra, observemos $w$ palabras hacia atrás y $w$ palabras hacia adelante. De esta manera, nos construimos un dataset para una tarea autosupervizada, que consiste en predecir la palabra central usando como features las palabras del contexto."
      ],
      "id": "f76c842a-bc4a-48a2-b611-c326202a162d"
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "0d650ea0-e850-4ac7-ba23-13af5e838e96"
      },
      "source": [
        "def slide_window(text, w):\n",
        "    doc = nlp(text)\n",
        "    tokens = [token.text for token in doc if not token.is_punct]\n",
        "    rows = []\n",
        "    for i in range(w, len(tokens)-w):\n",
        "        row = []\n",
        "        for j in range(-w, w+1):\n",
        "            if j == 0:\n",
        "                continue\n",
        "            row.append(tokens[i+j])\n",
        "        row.append(tokens[i])\n",
        "        rows.append(row)\n",
        "    cols = list(range(-w, 0)) + list(range(1, w+1)) + ['target']\n",
        "    df = pd.DataFrame(rows, columns=cols)\n",
        "    return df"
      ],
      "id": "0d650ea0-e850-4ac7-ba23-13af5e838e96",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "c974ba83-0b4e-4502-846a-330e78b61051",
        "outputId": "414adc11-0d70-4b2c-8530-0c637e492a6f"
      },
      "source": [
        "text = (\n",
        "'beautiful is better than ugly '\n",
        "'explicit is better than implicit '\n",
        "'simple is better than complex '\n",
        "'complex is better than complicated '\n",
        "'flat is better than nested '\n",
        "'sparse is better than dense '\n",
        ")\n",
        "\n",
        "print(\"Nuestro dataset construido:\")\n",
        "df = slide_window(text, w=2)\n",
        "df.head()"
      ],
      "id": "c974ba83-0b4e-4502-846a-330e78b61051",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Nuestro dataset construido:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>-2</th>\n",
              "      <th>-1</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>beautiful</td>\n",
              "      <td>is</td>\n",
              "      <td>than</td>\n",
              "      <td>ugly</td>\n",
              "      <td>better</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>is</td>\n",
              "      <td>better</td>\n",
              "      <td>ugly</td>\n",
              "      <td>explicit</td>\n",
              "      <td>than</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>better</td>\n",
              "      <td>than</td>\n",
              "      <td>explicit</td>\n",
              "      <td>is</td>\n",
              "      <td>ugly</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>than</td>\n",
              "      <td>ugly</td>\n",
              "      <td>is</td>\n",
              "      <td>better</td>\n",
              "      <td>explicit</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ugly</td>\n",
              "      <td>explicit</td>\n",
              "      <td>better</td>\n",
              "      <td>than</td>\n",
              "      <td>is</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          -2        -1         1         2    target\n",
              "0  beautiful        is      than      ugly    better\n",
              "1         is    better      ugly  explicit      than\n",
              "2     better      than  explicit        is      ugly\n",
              "3       than      ugly        is    better  explicit\n",
              "4       ugly  explicit    better      than        is"
            ]
          },
          "execution_count": 59,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a59d1f2f-e77c-4450-9d18-f0d0342c43d2"
      },
      "source": [
        "Ahora, para cada palabra asociamos un vector de dimensión dada (por ejemplo, 300), inicialmente con valores aleatorios. Luego entrenamos el modelo, ajustando los valores de los vectores de manera tal de mejorar las predicciones. Para una explicación más detallada del modelo, ver este [link](https://jalammar.github.io/illustrated-word2vec/)."
      ],
      "id": "a59d1f2f-e77c-4450-9d18-f0d0342c43d2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3a1f3942-b131-4329-ad7a-4cedfec3ac51"
      },
      "source": [
        "Veamos cómo funciona el modelo Word2Vec en español. Usaremos el modelo entrenado por Cristian Cardellino (ver [referencia](https://crscardellino.ar/resources/nlp/2016/02/06/spanish-billion-words-corpus-and-embeddings.html)), el cual fue entrenado utilizando más de mil millones de palabras."
      ],
      "id": "3a1f3942-b131-4329-ad7a-4cedfec3ac51"
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [],
        "id": "c0f011d0-9b66-4cf8-96fb-630f504ed4e5"
      },
      "source": [
        "def import_vectors():\n",
        "    from gensim.models import KeyedVectors\n",
        "    path = '/home/nahuel/Downloads/vectors/sbwce.wordvectors.all.bin'\n",
        "    vectors = KeyedVectors.load_word2vec_format(path, binary=True)\n",
        "    return vectors"
      ],
      "id": "c0f011d0-9b66-4cf8-96fb-630f504ed4e5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56cc44c8-b958-4ad0-a7b8-755937fc2f5e"
      },
      "source": [
        "vectors = import_vectors()"
      ],
      "id": "56cc44c8-b958-4ad0-a7b8-755937fc2f5e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5d1cb6a-9347-427a-b5b3-91a50db9930d",
        "outputId": "550c9240-3861-4520-81ae-f8b294eee42b"
      },
      "source": [
        "vectors.most_similar('asno')"
      ],
      "id": "b5d1cb6a-9347-427a-b5b3-91a50db9930d",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('burro', 0.8029149770736694),\n",
              " ('borrico', 0.7216041088104248),\n",
              " ('pollino', 0.7184825539588928),\n",
              " ('buey', 0.6988316178321838),\n",
              " ('camello', 0.6951327919960022),\n",
              " ('mulo', 0.6928654909133911),\n",
              " ('caballo', 0.6903840899467468),\n",
              " ('cebro', 0.6893405914306641),\n",
              " ('morueco', 0.6792659163475037),\n",
              " ('jumento', 0.6788411140441895)]"
            ]
          },
          "execution_count": 55,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5234e398-8e55-40ac-b21d-0d1627982702"
      },
      "source": [
        "## Aritmética de vectores\n",
        "\n",
        "Los vectores generados por Word2Vec permiten establecer ciertas relaciones aritméticas entre palabras. Por ejemplo, si a la palabra \"rey\" le resto la palabra \"hombre\", el resultado es similar al de restar \"mujer\" a \"reina\". Es decir,\n",
        "\n",
        "$$\n",
        "\\text{rey} - \\text{hombre} \\approx \\text{reina} - \\text{mujer},\n",
        "$$\n",
        "\n",
        "o bien,\n",
        "\n",
        "\n",
        "$$\n",
        "\\text{rey} + \\text{mujer} - \\text{hombre} \\approx \\text{reina}.\n",
        "$$\n",
        "\n",
        "(para más detalles, ver ecuación 4 en este [artículo](https://aclanthology.org/W14-1618/))."
      ],
      "id": "5234e398-8e55-40ac-b21d-0d1627982702"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ff5849f-5754-4164-a1ad-cd7c2d107414",
        "outputId": "960b2fbc-b71a-46ef-837f-1e041442b8ac"
      },
      "source": [
        "vectors.most_similar_cosmul(positive=['rey', 'mujer'], negative=['hombre'])"
      ],
      "id": "1ff5849f-5754-4164-a1ad-cd7c2d107414",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('reina', 0.9791632294654846),\n",
              " ('princesa', 0.9376253485679626),\n",
              " ('consorte', 0.9167759418487549),\n",
              " ('Isabel_de_Francia', 0.9086166620254517),\n",
              " ('Juana_de_Castilla', 0.9066565632820129),\n",
              " ('Margarita_I_de_Dinamarca', 0.8999430537223816),\n",
              " ('Constanza_de_Sicilia', 0.8989415764808655),\n",
              " ('Alfonso_V_de_León', 0.8985891342163086),\n",
              " ('Sancha_de_León', 0.8979201912879944),\n",
              " ('esposa', 0.8978815674781799)]"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2192ca24-dc2e-41dd-b2a4-f681749a1a9b"
      },
      "source": [
        "De forma similar, se pueden verificar otro tipo de asociaciones, como por ejemplo identificar términos no relacionados dentro de una lista"
      ],
      "id": "2192ca24-dc2e-41dd-b2a4-f681749a1a9b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3e28dcfb-f081-463d-9e4b-ccc88bc34db1",
        "outputId": "80831725-4470-423a-a4ca-f60cf32a193c"
      },
      "source": [
        "vectors.doesnt_match(['blanco', 'azul', 'rojo', 'argentina'])"
      ],
      "id": "3e28dcfb-f081-463d-9e4b-ccc88bc34db1",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'argentina'"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf65cb90-835e-4100-b4da-ab2accad7e4d"
      },
      "source": [
        "Existen muchos tipos de embeddings predictivos. Algunos de ellos han sido entrenados con texto en español y pueden encontrarse en este [repositorio](https://github.com/dccuchile/spanish-word-embeddings)."
      ],
      "id": "cf65cb90-835e-4100-b4da-ab2accad7e4d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "tags": [],
        "id": "04895e25-40a4-4cea-9e4d-7e374c6bed6d"
      },
      "source": [
        "# Detección de tópicos\n",
        "\n",
        "-----------\n",
        "\n",
        "Una de las tareas más frecuentes en el procesamiento del lenguaje natural es la detección automática de tópicos en un conjunto de documentos. Este problema puede formularse dentro del aprendizaje supervisado si contamos con etiquetas para ciertos documentos. Sin embargo, es más habitual formularlo como un problema de aprendizaje no supervisado"
      ],
      "id": "04895e25-40a4-4cea-9e4d-7e374c6bed6d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04b8fdf6-418a-4448-8e1c-696dc3d5504d"
      },
      "source": [
        "Uno de los métodos más empleados para detección de tópicos se conoce como análisis de semántica latente, o [LSA](https://en.wikipedia.org/wiki/Latent_semantic_analysis), por sus siglas en inglés. Básicamente consiste en utilizar la técnica de descomposición en valores singulares sobre la matriz de términos-documentos.\n",
        "\n",
        "Para más detalles de implementación de LSA usando `gensim`, ver este [tutorial](https://www.datacamp.com/community/tutorials/discovering-hidden-topics-python)."
      ],
      "id": "04b8fdf6-418a-4448-8e1c-696dc3d5504d"
    }
  ]
}